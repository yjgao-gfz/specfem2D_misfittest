{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('run many events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###here the mesh I used 7500 m which is very dense mesh with over 900000 elements and caused large burdern for the smoothing,\n",
    "### thus decrease to 15 km is ok for P wave in the mantle with 2 elements to sample the wavelength or equal to 1 element for the crust \n",
    "### just for tests and illustration for my thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####run the jupyter notebook for the tomography model \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "PROJECT='/data2/yjgao/data/DATASET/package_forthesis_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_forward(dir_run,dir_event,simtype,modelfile,dir_storesyn):\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    CHECK_FOLDER = os.path.isdir(dir_storesyn)\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(dir_storesyn)\n",
    "    from shutil import copyfile\n",
    "    from distutils.dir_util import copy_tree\n",
    "    copyfile(modelfile,'DATA/profile.xyz')\n",
    "    copyfile('../run_this_example.sh','run_this_example.sh')\n",
    "    os.system('ln -s specfem2d-devel/utils/change_simulation_type.pl')\n",
    "    os.system('perl change_simulation_type.pl -f') \n",
    "    print('start forward modelling',dir_event)\n",
    "    os.system('bash run_this_example.sh')\n",
    "    print('finished forward modelling')\n",
    "    ###store the synthetics to the dir observed or syn for backup\n",
    "    copy_tree('OUTPUT_FILES',dir_storesyn)\n",
    "    os.chdir(dir_run)\n",
    "    #print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####compare smoothed model with 1D model \n",
    "modelfile=PROJECT+'MODELS/profile_obs.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "#launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT2','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT3','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT4','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT5','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT6','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT7','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT8','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT9','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT10','forward',modelfile,'REF_SEM')\n",
    "#launch_forward(PROJECT,'EVENT11','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT12','forward',modelfile,'REF_SEM')\n",
    "launch_forward(PROJECT,'EVENT13','forward',modelfile,'REF_SEM')\n",
    "print (datetime.now()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "#launch_forward(PROJECT,'EVENT1','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT2','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT3','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT4','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT5','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT6','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT7','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT8','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT9','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT10','forward',modelfile,'REF_SEM_smooth')\n",
    "#launch_forward(PROJECT,'EVENT11','forward',modelfile,'REF_SEM_smooth')\n",
    "launch_forward(PROJECT,'EVENT12','forward',modelfile,'REF_SEM_smooth')\n",
    "launch_forward(PROJECT,'EVENT13','forward',modelfile,'REF_SEM_smooth')\n",
    "print (datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source(dir_source):\n",
    "    file1 = open(dir_source, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    x=np.float(Lines[2].split('=')[1].split('#')[0])\n",
    "    z=np.float(Lines[3].split('=')[1].split('#')[0])\n",
    "    return x,z\n",
    "def read_receiver(dir_receiver):\n",
    "    import re\n",
    "    file1 = open(dir_receiver, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    number=len(Lines)\n",
    "    receiver_station=[]\n",
    "    receiver_net=[]\n",
    "    receiver_x=np.zeros(number)\n",
    "    receiver_z=np.zeros(number)\n",
    "    for _i, line in enumerate(Lines):\n",
    "        station,network,x,z,nun,nun=re.findall(r'\\S+', line)\n",
    "        #print(station)\n",
    "        receiver_station.append(station)\n",
    "        receiver_net.append(network)\n",
    "        receiver_x[_i]=np.float(x)\n",
    "        receiver_z[_i]=np.float(z)\n",
    "    return receiver_station,receiver_net,receiver_x,receiver_z\n",
    "\n",
    "def launch_adjoint(dir_run,dir_event,dir_storesyn,modelfile):\n",
    "    from datetime import datetime\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    from shutil import copyfile\n",
    "    import shutil\n",
    "    from distutils.dir_util import copy_tree\n",
    "    #copyfile(modelfile,'DATA/profile.xyz')\n",
    "    #os.remove('change_simulation_type.pl')\n",
    "    #s.system('ln -s /home/yjgao/install/specfem2d-devel/utils/change_simulation_type.pl')\n",
    "    #s.system('perl change_simulation_type.pl -f') \n",
    "    copyfile(modelfile,'DATA/profile.xyz')\n",
    "    copyfile('../run_this_example_kernel.sh','run_this_example_kernel.sh')\n",
    "    print('start adjoint modelling',dir_event)\n",
    "    os.system('bash run_this_example_kernel.sh')\n",
    "    print('finished adjoint modelling')\n",
    "    ###store the synthetics to the dir observed or syn for backup\n",
    "    #!cat OUTPUT_FILES/proc000*_rho_kappa_mu_kernel.dat > OUTPUT_FILES/rho_kappa_mu_kernel.dat\n",
    "    ####prepared to \n",
    "    os.system('cat OUTPUT_FILES/proc000*_rhop_alpha_beta_kernel.dat > OUTPUT_FILES/rho_alpha_beta_kernel.dat')\n",
    "    shutil.copy('OUTPUT_FILES/rho_alpha_beta_kernel.dat', dir_storesyn)\n",
    "    os.chdir(dir_run)\n",
    "def adj_calculate(dir_run,dir_event,dir_obs,dir_syn,adj_src_type,stationpath):\n",
    "    import obspy\n",
    "    import pyadjoint\n",
    "    import numpy as np\n",
    "\n",
    "    from obspy.taup import TauPyModel\n",
    "    from obspy.core.trace import Trace\n",
    "    ##guarantee to change back to the project directory\n",
    "    os.chdir(dir_run)\n",
    "    os.chdir(dir_event)\n",
    "    ##### read source locations\n",
    "    source_x,source_depth_in_m=read_source('DATA/SOURCE')\n",
    "    ##### read receiver locations\n",
    "    receiver_station,receiver_net,receiver_x,receiver_z=read_receiver(stationpath)\n",
    "    import toml\n",
    "    Misfits_all=0.0\n",
    "    fh = open(dir_syn+'misfit.txt','w')\n",
    "    for i, station_name in enumerate(receiver_station):\n",
    "        #print(i)\n",
    "        #print(np.int(i))\n",
    "        distance=np.abs(receiver_x[i]-source_x)/111.0/1000.0\n",
    "        model = TauPyModel(model=\"iasp91\")\n",
    "        # adapted \n",
    "        arrivals = model.get_travel_times(source_depth_in_km=-source_depth_in_m/1000,distance_in_degree=distance) \n",
    "        time=arrivals[0].time\n",
    "        if time > 400-15 :\n",
    "            window=[]\n",
    "        else:\n",
    "            window=[[time-3,time+14]]\n",
    "        print(window)\n",
    "        network=receiver_net[i]\n",
    "        station=receiver_station[i]\n",
    "        data_hetero=np.loadtxt(dir_syn+network+'.'+station+'.BXZ'+'.semd')\n",
    "        data_hetero_new=data_hetero.swapaxes(0,1)\n",
    "    #from obspy.core.trace import Trace\n",
    "        tr=Trace(data=data_hetero_new[1])\n",
    "        tr.stats.delta=0.02  ###here should be adapted to the DATA/Par_file\n",
    "        tr.stats.network = network\n",
    "        tr.stats.station = station\n",
    "        tr.stats.channel = 'BXZ'\n",
    "        data_hetero_obs=np.loadtxt(dir_obs+network+'.'+station+'.BXZ'+'.semd')                           \n",
    "        data_hetero_new_obs=data_hetero_obs.swapaxes(0,1)\n",
    "        tr_obs=Trace(data=data_hetero_new_obs[1])\n",
    "        tr_obs.stats.delta=0.02\n",
    "        tr_obs.stats.network = network\n",
    "        tr_obs.stats.station = station\n",
    "        tr_obs.stats.channel = 'BXZ'\n",
    "        ###here input a minum and maximum period\n",
    "        configure=pyadjoint.config.Config(4,100,measure_type='dt')\n",
    "        print(configure)\n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr, min_period=4, max_period=100,\n",
    "        config=configure, window=window, plot=True);\n",
    "        Misfits_all+=adj.misfit\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXZ'+'.adj',format=\"SPECFEM\", time_offset=-6)\n",
    "        \n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr, min_period=4, max_period=100,\n",
    "        config=configure, window=[], plot=True);\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXY'+'.adj',format=\"SPECFEM\", time_offset=-6)\n",
    "        \n",
    "        data_hetero=np.loadtxt(dir_syn+network+'.'+station+'.BXX'+'.semd')\n",
    "        data_hetero_new=data_hetero.swapaxes(0,1)\n",
    "        tr=Trace(data=data_hetero_new[1])\n",
    "        tr.stats.delta=0.02\n",
    "        tr.stats.network = network\n",
    "        tr.stats.station = station\n",
    "        tr.stats.channel = 'BXX'\n",
    "        data_hetero_obs=np.loadtxt(dir_obs+network+'.'+station+'.BXX' +'.semd')                           \n",
    "        data_hetero_new_obs=data_hetero_obs.swapaxes(0,1)\n",
    "        tr_obs=Trace(data=data_hetero_new_obs[1])\n",
    "        tr_obs.stats.delta=0.02\n",
    "        tr_obs.stats.network = network\n",
    "        tr_obs.stats.station = station\n",
    "        tr_obs.stats.channel = 'BXX'\n",
    "        adj=pyadjoint.calculate_adjoint_source(\n",
    "        adj_src_type=adj_src_type, observed=tr_obs, synthetic=tr, min_period=4, max_period=100,\n",
    "        config=configure, window=window, plot=True);\n",
    "        print(adj)\n",
    "        adj.write(filename='SEM/'+network+'.'+station+'.BXX'+'.adj',format=\"SPECFEM\", time_offset=-6)\n",
    "    fh.write(dir_syn+' ' + dir_event +' ' + str(Misfits_all)+\"\\n\")\n",
    "    fh.close()\n",
    "    os.chdir(dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT12','EVENT13']\n",
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "print('tf')\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_smooth/','tf_phase_misfit','DATA/STATIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT12','EVENT13']\n",
    "for _i, event in enumerate(list_events):\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_smooth/',modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    print(\"Error importing pyplot from matplotlib, please install matplotlib package first...\")\n",
    "    sys.tracebacklimit=0\n",
    "    raise Exception(\"Importing matplotlib failed\")\n",
    "###controlling the speed for smoothing!\n",
    "def grid(x, y, z, resX=200, resY=200):\n",
    "    \"\"\"\n",
    "    Converts 3 column data to matplotlib grid\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import griddata\n",
    "    #from scipy.interpolate import griddata\n",
    "\n",
    "    xi = np.linspace(min(x), max(x), resX)\n",
    "    yi = np.linspace(min(y), max(y), resY)\n",
    "\n",
    "    # mlab version\n",
    "    Z = griddata((x, y), z, (xi[None,:], yi[:,None]), method='linear')\n",
    "    # scipy version\n",
    "    #Z = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n",
    "\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "    return X, Y, Z\n",
    "\n",
    "def plot_kernels(filename,show=False,EVENT='EVENT',total_max=1e-10,figurename='test.png'):\n",
    "    \"\"\"\n",
    "    plots ASCII kernel file\n",
    "    \"\"\"\n",
    "    #print \"plotting kernel file: \",filename\n",
    "    #print \"\"\n",
    "\n",
    "    data = np.loadtxt(filename)\n",
    "\n",
    "    # checks data\n",
    "    if data.ndim != 2:\n",
    "        #print \"Error: wrong data dimension for kernel file\",data.ndim\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data dimension\")\n",
    "\n",
    "    # checks array\n",
    "    if len(data[1,:]) != 5:\n",
    "        #print \"data shape  : \",data.shape\n",
    "        #print \"data lengths: \",len(data[:,1]),len(data[1,:])\n",
    "        #print \"Error: wrong data format for kernel file\",data.shape\n",
    "        sys.tracebacklimit=0\n",
    "        raise Exception(\"Invalid data format\")\n",
    "\n",
    "    # splits up data\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "\n",
    "    #print \"dimensions:\"\n",
    "    #print \"  x-range min/max = %f / %f\" % (x.min(), x.max())\n",
    "    #print \"  y-range min/max = %f / %f\" % (y.min(), y.max())\n",
    "    #print \"\"\n",
    "\n",
    "    z1 = data[:,2] # e.g. rho\n",
    "    z2 = data[:,3] # e.g. alpha\n",
    "    z3 = data[:,4] # e.g. beta\n",
    "\n",
    "    # names like\n",
    "    #   rhop_alpha_beta_kernel.dat\n",
    "    # or\n",
    "    #   proc000000_rhop_alpha_beta_kernel.dat\n",
    "    name = os.path.basename(filename)\n",
    "\n",
    "    name_kernels = str.split(name,\"_\")\n",
    "    if len(name_kernels) == 4:\n",
    "        kernel1 = 'K_' + name_kernels[0] # rhop\n",
    "        kernel2 = 'K_' + name_kernels[1] # alpha\n",
    "        kernel3 = 'K_' + name_kernels[2] # beta\n",
    "    elif len(name_kernels) == 5:\n",
    "        kernel1 = 'K_' + name_kernels[1]\n",
    "        kernel2 = 'K_' + name_kernels[2]\n",
    "        kernel3 = 'K_' + name_kernels[3]\n",
    "    else:\n",
    "        kernel1 = 'K_1'\n",
    "        kernel2 = 'K_2'\n",
    "        kernel3 = 'K_3'\n",
    "\n",
    "    #print \"statistics:\"\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel1,z1.min(),z1.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel2,z2.min(),z2.max())\n",
    "    #print \"  %12s : min/max = %e / %e\" % (kernel3,z3.min(),z3.max())\n",
    "    #print \"\"\n",
    "\n",
    "    #total_max = 0.1*abs(z2).max()\n",
    "    print(total_max)\n",
    "    #print \"  data max = \",total_max\n",
    "    #print \"\"\n",
    "    #print(total_max)\n",
    "    #total_max = 1.e-8\n",
    "\n",
    "    # setup figure (with 3 subplots)\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1,figsize=(10,10))\n",
    "\n",
    "    for i,ax in enumerate(axes.flat,start=1):\n",
    "        # top\n",
    "        if i == 1:\n",
    "            X, Y, Z = grid(x,y,z1)\n",
    "            ax.set_title(\"Kernels\")\n",
    "            ax.set_ylabel('RHO')\n",
    "        elif i == 2:\n",
    "            X, Y, Z = grid(x,y,z2)\n",
    "            ax.set_ylabel('VP')\n",
    "        elif i == 3:\n",
    "            X, Y, Z = grid(x,y,z3)\n",
    "            ax.set_ylabel('VS')\n",
    "\n",
    "        #colormap = 'jet'\n",
    "        colormap = 'RdBu'\n",
    "        print(np.max(np.abs(Z)))\n",
    "        im = ax.imshow(Z/np.max(np.abs(Z)),vmax=total_max, vmin=-total_max,extent=[0,25*111,0,800],cmap=colormap)\n",
    "        ax.plot([11*111,14*111,14*111,11*111,11*111],[140,140,230,230,140],'black')\n",
    "        ax.plot(666,121,'o',color='black') #1\n",
    "        ax.plot(666,221,'o',color='black') #2\n",
    "        ax.plot(666,321,'o',color='black') #3\n",
    "        ax.plot(1400,321,'o',color='black') #4\n",
    "        ax.plot(2200,121,'o',color='black') #5 \n",
    "        ax.plot(2200,221,'o',color='black') #6 \n",
    "        ax.plot(2200,321,'o',color='black') #7\n",
    "        ax.plot(1100,321,'o',color='black') #8\n",
    "        ax.plot(1700,321,'o',color='black') #9\n",
    "        ax.plot(1100,221,'o',color='black') #10\n",
    "        ax.plot(1700,221,'o',color='black') #11\n",
    "        ax.plot(1100,121,'o',color='black') #13\n",
    "        ax.plot(1700,121,'o',color='black') #12\n",
    "               # im = ax.imshow(Z,vmax=total_max, vmin=total_min,extent=[0,45*111,0,1000],cmap=colormap.reversed())\n",
    "      #  ax.plot([18*111,25*111,25*111,18*111,18*111],[500,500,660,660,500],'black')\n",
    "      #  ax.plot([0,45*111],[410,410],'black')\n",
    "      #  ax.plot([0,45*111],[660,660],'black')\n",
    "      #  ax.text(10,390,'410 km', fontsize=9)\n",
    "      #  ax.text(10,640,'660 km', fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "        #ax.set_xlim(2000000,3000000)\n",
    "        #ax.set_ylim(-200000,0)\n",
    "    # moves plots together\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    #plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)\n",
    "    \n",
    "    # colorbar\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    #plt.xlim(2500000,3000000)\n",
    "    #plt.ylim(-200000,0)\n",
    "    #fig.colorbar(im, ax=axes.ravel().tolist(),orientation='horizontal')\n",
    "\n",
    "    # show the figure\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    # saves kernel figure as file\n",
    "    dir = os.path.dirname('.')\n",
    "    name_without_ending = str.split(name,\".\")[0]\n",
    "    outfile = dir + \"/\" + name_without_ending + \".png\"\n",
    "    fig.savefig(figurename, format=\"png\",  dpi=300)\n",
    "\n",
    "    #print \"*****\"\n",
    "    #print \"plotted file: \",outfile\n",
    "    #print \"*****\"\n",
    "    #print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gradient(dir_gradient):\n",
    "    gradient=np.loadtxt(dir_gradient)\n",
    "    x=gradient.T[0]\n",
    "    z=gradient.T[1]\n",
    "    rho=gradient.T[2]\n",
    "    alpha=gradient.T[3]\n",
    "    belta=gradient.T[4]\n",
    "    return x,z,rho,alpha,belta\n",
    "def read_modelfile(modelfile):\n",
    "    model=np.loadtxt('MODELS/'+modelfile)\n",
    "    x_new=model.T[0]\n",
    "    z_new=model.T[1]\n",
    "    vp=model.T[2]\n",
    "    vs=model.T[3]\n",
    "    rho=model.T[4]\n",
    "    return x_new,z_new,vp,vs,rho\n",
    "def sum_gradient(events,iteration,smoothsrc='True'):\n",
    "    if smoothsrc=='True':\n",
    "        gradient=np.loadtxt(list_events[0]+'/REF_SEM_'+iteration+'/'+'rho_alpha_beta_kernelsmsrc.dat')\n",
    "        pendix='rho_alpha_beta_kernelsmsrc.dat'\n",
    "    else:\n",
    "        gradient=np.loadtxt(list_events[0]+'/REF_SEM_'+iteration+'/'+'rho_alpha_beta_kernel.dat')\n",
    "        pendix='rho_alpha_beta_kernel.dat'\n",
    "    gradient_sum=np.zeros((gradient.shape))\n",
    "    for _i,gradient in enumerate(list_events):\n",
    "        x,z,rho,alpha,belta=read_gradient(list_events[_i]+'/REF_SEM_'+iteration+'/'+pendix)\n",
    "        gradient_sum.T[2]+=rho\n",
    "        gradient_sum.T[3]+=alpha\n",
    "        gradient_sum.T[4]+=belta\n",
    "    gradient_sum.T[0]=x\n",
    "    gradient_sum.T[1]=z\n",
    "    np.savetxt(iteration+'_summed_gradient.dat',gradient_sum)\n",
    "    return gradient\n",
    "def cut_source_region_from_gradient_gauss(EVENT:str,iteration:str, radius_to_cut: float):\n",
    "    \"\"\"\n",
    "    Sources often show unreasonable sensitivities. This function\n",
    "    brings the value of the gradient down to zero for that region.\n",
    "    I recommend doing this before smoothing.\n",
    "    \n",
    "    :param mesh: Path to the mesh\n",
    "    :type mesh: str\n",
    "    :param source_location: Source latitude, longitude and depth\n",
    "    :type source_location: dict\n",
    "    :param radius_to_cut: Radius to cut in km\n",
    "    :type radius_to_cut: float\n",
    "    \"\"\"\n",
    "    import lasif\n",
    "    dir_gradient= EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernel.dat'\n",
    "    #print(mesh)\n",
    "    x,z,rho,alpha,belta=read_gradient(dir_gradient)\n",
    "    gradient=np.loadtxt(dir_gradient)\n",
    "    s_x,s_z=read_source(EVENT+'/DATA/SOURCE')\n",
    "    dist = np.sqrt( (x - s_x) ** 2 + (z - s_z) ** 2)\n",
    "    \n",
    "    ####here we implement 1-Gaussian smoothing the source\n",
    "    #distance_smooth = (x[k] * np.ones(number_nodes) - x) ** 2 + (y[k] * np.ones(number_nodes) - y)** 2 + (z[k] * np.ones(number_nodes) - z) ** 2\n",
    "    distance_smooth1 = np.exp(-dist/ (2 * radius_to_cut ** 2))\n",
    "    print(dist.max(),distance_smooth1.max())\n",
    "    distance_smooth1 = distance_smooth1 / distance_smooth1.max()\n",
    "    print(distance_smooth1.max())\n",
    "    alpha_new=alpha*(1-distance_smooth1)\n",
    "    belta_new=belta*(1-distance_smooth1)\n",
    "    rho_new=rho*(1-distance_smooth1)\n",
    "    gradient_new=gradient.copy()\n",
    "    smooth_dist=gradient.copy()\n",
    "    gradient_new.T[2]=rho_new\n",
    "    gradient_new.T[3]=alpha_new\n",
    "    gradient_new.T[4]=belta_new\n",
    "    smooth_dist.T[2]=1-distance_smooth1\n",
    "    smooth_dist.T[3]=1-distance_smooth1\n",
    "    smooth_dist.T[4]=1-distance_smooth1\n",
    "    np.savetxt(EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernelsmsrc.dat',gradient_new)\n",
    "    np.savetxt(EVENT+ '/REF_SEM_' +iteration+'/rho_alpha_beta_kernelsdist.dat',smooth_dist)\n",
    "    \n",
    "\n",
    "\n",
    "def smooth_cupy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix): \n",
    "      ##input a numpy array and using asarray move to the gpu device\n",
    "      import cupy as cp\n",
    "      Velocity_matrix=cp.asarray(Velocity_matrix)\n",
    "      xyz_matrix=cp.asarray(xyz_matrix)\n",
    "      Vpv_smooth_all=cp.zeros((3,iterations),dtype=float)  \n",
    "      #t1 = datetime.datetime.now()       \n",
    "      for i in range(iterations):  \n",
    "            distancex=(cp.square(xyz_matrix[0][i]-xyz_matrix[0]))/(2*sigmaxy*sigmaxy)\n",
    "            distancey=(cp.square(xyz_matrix[1][i]-xyz_matrix[1]))/(2*sigmaxy*sigmaxy)\n",
    "            #distancez=(cp.square(xyz_matrix[2][i]-xyz_matrix[2]))/(2*sigmaz*sigmaz)\n",
    "            #print(distance)\n",
    "            distance=distancex+distancey\n",
    "            distance=cp.exp(-distance)\n",
    "            distance= cp.divide(distance,cp.sum(distance))\n",
    "            #print(distance)\n",
    "            Vpv_smooth_all[0][i] = cp.dot(distance, Velocity_matrix[0])\n",
    "            Vpv_smooth_all[1][i] = cp.dot(distance, Velocity_matrix[1])\n",
    "            Vpv_smooth_all[2][i] = cp.dot(distance, Velocity_matrix[2])\n",
    "      #print(t2-t1)\n",
    "      # move array from gpu device to the host\n",
    "      return cp.asnumpy(Vpv_smooth_all)\n",
    "\n",
    "def smooth_numpy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix): \n",
    "      ##input a numpy array and using asarray move to the gpu device\n",
    "     # import cupy as cp\n",
    "      Velocity_matrix=np.asarray(Velocity_matrix)\n",
    "      xyz_matrix=np.asarray(xyz_matrix)\n",
    "      Vpv_smooth_all=np.zeros((3,iterations),dtype=float)  \n",
    "      #t1 = datetime.datetime.now()       \n",
    "      for i in range(iterations):  \n",
    "            distancex=(np.square(xyz_matrix[0][i]-xyz_matrix[0]))/(2*sigmaxy*sigmaxy)\n",
    "            distancey=(np.square(xyz_matrix[1][i]-xyz_matrix[1]))/(2*sigmaxy*sigmaxy)\n",
    "            #distancez=(cp.square(xyz_matrix[2][i]-xyz_matrix[2]))/(2*sigmaz*sigmaz)\n",
    "            #print(distance)\n",
    "            distance=distancex+distancey\n",
    "            distance=np.exp(-distance)\n",
    "            distance= np.divide(distance,np.sum(distance))\n",
    "            #print(distance)\n",
    "            Vpv_smooth_all[0][i] = np.dot(distance, Velocity_matrix[0])\n",
    "            Vpv_smooth_all[1][i] = np.dot(distance, Velocity_matrix[1])\n",
    "            Vpv_smooth_all[2][i] = np.dot(distance, Velocity_matrix[2])\n",
    "      #print(t2-t1)\n",
    "      # move array from gpu device to the host\n",
    "      return Vpv_smooth_all\n",
    "\n",
    "\n",
    "def smooth_compat(x,y,Vp,Vs,rho,sigmaxy,sigmaz):   \n",
    "    number_nodes=x.shape[0]\n",
    "    print(number_nodes)\n",
    "    V_smooth_all=np.zeros((3,number_nodes),dtype=float)\n",
    "    xyz_matrix=np.zeros((2,number_nodes),dtype=float)\n",
    "    xyz_matrix[0]=x\n",
    "    xyz_matrix[1]=y\n",
    "    Velocity_matrix=np.zeros((3,number_nodes),dtype=float)\n",
    "    Velocity_matrix[0]=Vp\n",
    "    Velocity_matrix[1]=Vs\n",
    "    Velocity_matrix[2]=rho\n",
    "    iterations=number_nodes\n",
    "    #Vpv1_smooth_all=np.zeros((5,206313),dtype=float)\n",
    "    test_cupy=smooth_cupy_new(xyz_matrix,iterations,sigmaxy,sigmaz,Velocity_matrix)\n",
    "    return test_cupy\n",
    "\n",
    "def smooth_kernel(gradient,iteration,sigmax,sigmaz):\n",
    "    x,z,rho_kernel,alpha_kernel,belta_kernel=read_gradient(gradient)\n",
    "    smooth_format=smooth_compat(x,z,alpha_kernel,belta_kernel,rho_kernel,sigmax,sigmaz)\n",
    "    smooth_kernel=np.zeros((5,x.shape[0]))\n",
    "    smooth_kernel[0]=x\n",
    "    smooth_kernel[1]=z\n",
    "    ####follow the sequence of specfem2d for the gradient field rho, p, s\n",
    "    smooth_kernel[2]=smooth_format[2]\n",
    "    smooth_kernel[3]=smooth_format[0]\n",
    "    smooth_kernel[4]=smooth_format[1]\n",
    "    np.savetxt(iteration+'_summed_gradient_smooth.dat',smooth_kernel.T)\n",
    "\n",
    "    \n",
    "###since most of the artifacts around the source are removed, the remainings are derived from the surface beneath the receivers\n",
    "### Because remove the artifacts around receivers are much more expensive, it would be great to desighn a function to suppress the \n",
    "### gradients with depth\n",
    "def remove_artifacts(gradient, iteration, supress_min, supress_depth):\n",
    "    x,z,rho_kernel,alpha_kernel,belta_kernel=read_gradient(gradient)\n",
    "    suppress=1-np.exp(-z**2/ (2 * supress_depth ** 2))+supress_min\n",
    "    rho_kernel_suppress=suppress*rho_kernel\n",
    "    alpha_kernel_suppress=suppress*alpha_kernel\n",
    "    belta_kernel_suppress=suppress* belta_kernel\n",
    "    supress_kernel=np.zeros((5,x.shape[0]))\n",
    "    supress_kernel[0]=x\n",
    "    supress_kernel[1]=z\n",
    "    ####follow the sequence of specfem2d for the gradient field rho, p, s\n",
    "    supress_kernel[2]=rho_kernel_suppress\n",
    "    supress_kernel[3]=alpha_kernel_suppress\n",
    "    supress_kernel[4]=belta_kernel_suppress\n",
    "    plt.plot(z,suppress,'o')\n",
    "    np.savetxt(iteration+'_summed_gradient_supress.dat',supress_kernel.T)\n",
    "def interpolate_kernel_format(gradient,x_new,z_new):\n",
    "    x,z,rho_kernel,alpha_kernel,belta_kernel=read_gradient(gradient)\n",
    "    from scipy.interpolate import griddata\n",
    "    alpha_kernel_newformat=griddata((x,z),alpha_kernel,(x_new,z_new),method='cubic')\n",
    "    belta_kernel_newformat=griddata((x,z),belta_kernel,(x_new,z_new),method='cubic')\n",
    "    rho_kernel_newformat=griddata((x,z),rho_kernel,(x_new,z_new),method='cubic')\n",
    "###here because of the nan when using cubic at the boundary points\n",
    "    nan_index=np.argwhere(np.isnan(alpha_kernel_newformat))\n",
    "    alpha_kernel_newformat[nan_index]=griddata((x,z),alpha_kernel,(x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    belta_kernel_newformat[nan_index]=griddata((x,z),belta_kernel,(x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    rho_kernel_newformat[nan_index]  =griddata((x,z),rho_kernel,  (x_new[nan_index],z_new[nan_index]),method='nearest')\n",
    "    gradient_new=np.zeros((3,x_new.shape[0]))\n",
    "    gradient_new[2]=rho_kernel_newformat\n",
    "    gradient_new[0]=alpha_kernel_newformat\n",
    "    gradient_new[1]=belta_kernel_newformat\n",
    "    \n",
    "    return gradient_new\n",
    "def model_update_steepest(gradient,model,perturb_Vp,perturb_Vs,perturb_rho):\n",
    "    model=np.loadtxt(model)\n",
    "    x_new=model.T[0]\n",
    "    z_new=model.T[1]\n",
    "    vp=model.T[2]\n",
    "    vs=model.T[3]\n",
    "    rho=model.T[4]\n",
    "    ##through this, reforge the shape from rho, vp and vs to vp vs and rho, and interpolate\n",
    "    gradient=interpolate_kernel_format(gradient,x_new,z_new)\n",
    "    gradient_Vp_abs=np.absolute(gradient[0]) \n",
    "    gradient_Vs_abs=np.absolute(gradient[1]) \n",
    "    gradient_rho_abs=np.absolute(gradient[2]) \n",
    "\n",
    "    print('maxgrdientVp','maxgrdientVs','maxgrdientrho')\n",
    "    print(np.max(gradient_Vp_abs),np.max(gradient_Vs_abs),np.max(gradient_rho_abs))  \n",
    "    alpha_Vp=perturb_Vp*vp/gradient_Vp_abs \n",
    "    alpha_Vs=perturb_Vs*vs/gradient_Vs_abs \n",
    "    alpha_rho=perturb_rho*rho/gradient_rho_abs\n",
    "    step_Vp= np.min(alpha_Vp)\n",
    "    step_Vs= np.min(alpha_Vs)\n",
    "    step_rho= np.min(alpha_rho)\n",
    "    print('stepVp','stepVs','steprho')\n",
    "    print( step_Vp,step_Vs,step_rho)\n",
    "    print('variationmax for Vp,  Vs, Vsh,rho')\n",
    "    print( np.max(np.abs(step_Vp*gradient[0])),np.max(np.abs(step_Vs*gradient[1])),np.max(np.abs(step_rho*gradient[2])))\n",
    "    VpNew= model.T[2] - step_Vp*gradient[0]\n",
    "    VsNew= model.T[3] - step_Vs*gradient[1]\n",
    "    rhoNew= model.T[4] - step_rho*gradient[2]\n",
    "    return x_new,z_new,VpNew,VsNew,rhoNew\n",
    "\n",
    "def generate_newmodel(vp,vs,rho,x,z,newfile,newfile_noheader):  \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    maxx=45*111*1000\n",
    "    minx=0\n",
    "    maxz=0\n",
    "    minz=-1000*1000\n",
    "    nx = 800 # Number of sampling points in x direction\n",
    "    nz = 200  # Number of sampling points in z direction\n",
    "    spacing_x = (maxx-minx)/(nx-1)\n",
    "    spacing_z = (maxz-minz)/(nz-1)\n",
    "    print (\"space x space z\", spacing_x,spacing_z)\n",
    "    fo = open(newfile, \"w+\") # Name of the file it has to be set in the Par_file\n",
    "    fo2 = open(newfile_noheader, \"w+\")\n",
    "    print( \"Name of the file: \", fo.name)\n",
    "    orig_x=0\n",
    "    orig_z=0\n",
    "    end_x=maxx\n",
    "    end_z=maxz\n",
    "    line1 = str(orig_x)+\" \"+str(orig_z)+\" \"+str(end_x)+\" \"+str(end_z)+\"\\n\"\n",
    "    line2 = str(spacing_x)+\" \"+str(spacing_z)+\"\\n\"\n",
    "    line3 = str(nx)+\" \"+str(nz)+\"\\n\"\n",
    "# line4 : vpMin vpMax vsMin vsMax rhoMin rhoMax\n",
    "    line4 = str(min(vp))+\" \"+str(max(vp))+\" \"+str(min(vs))+\" \"+str(max(vs))+\" \"+str(min(rho))+\" \"+str(max(rho))+\"\\n\"\n",
    "# Write a line at the end of the file.\n",
    "    fo.write(line1)\n",
    "    fo.write(line2)\n",
    "    fo.write(line3)\n",
    "    fo.write(line4)\n",
    "    m=x.shape[0]\n",
    "    for i in np.arange(0,m):\n",
    "        lineToWrite= str(x[i])+\" \"+str(z[i])+\" \"+str(vp[i])+\" \"+str(vs[i])+\" \"+str(rho[i])+\"\\n\"\n",
    "        fo.write(lineToWrite)\n",
    "        fo2.write(lineToWrite)\n",
    "    fo.close()\n",
    "    fo.close()\n",
    "def retrieve_misfit(list_events,ITERATION):\n",
    "    import re\n",
    "    ALL_misfit=0\n",
    "    for _i, event in enumerate(list_events):\n",
    "        print(event)\n",
    "        file1 = open(event+'/'+'REF_SEM_'+ITERATION+'/misfit.txt', 'r')\n",
    "        Lines = file1.readlines()\n",
    "        path,event,misfit=re.findall(r'\\S+', Lines[0])\n",
    "        print(misfit)\n",
    "        ALL_misfit+=float(misfit)\n",
    "    return ALL_misfit\n",
    "        \n",
    "def assemble_lbfgs(listofgradients,listofmodels):\n",
    "    import h5py\n",
    "    from lasif import function_store\n",
    "    from scipy.interpolate import griddata\n",
    "    x,z,rho,alpha,belta=read_gradient(listofgradients[0])\n",
    "    x_new,z_new,vp,vs,rho=read_modelfile(listofmodels[0])\n",
    "    fields=['vp','vs','rho']\n",
    "    ###becareful the shape of model and gradient is differen\n",
    "    g=np.zeros((len(listofgradients),3,x_new.shape[0]))\n",
    "    m=np.zeros((len(listofgradients),3,x_new.shape[0]))\n",
    "    modelupdate=np.zeros((3,x_new.shape[0]))\n",
    "    for i in np.arange(len(listofgradients)):\n",
    "        gradient=interpolate_kernel_format(listofgradients[i],x_new,z_new)\n",
    "        x_new,z_new,vp,vs,rho =read_modelfile(listofmodels[i])\n",
    "        model=np.zeros((3,x_new.shape[0]))\n",
    "        model[0]=vp\n",
    "        model[1]=vs \n",
    "        model[2]=rho\n",
    "        g[i]=gradient\n",
    "        m[i]=model\n",
    "    g=np.swapaxes(g,0,1)\n",
    "    m=np.swapaxes(m,0,1)\n",
    "    \n",
    "    ##swap axes for loop the fields\n",
    "    for _i, field in enumerate(fields):\n",
    "        modelupdate[_i]=function_store.LBFGS_n_order(g[_i],m[_i])\n",
    "    return modelupdate\n",
    "def model_update_lbfgs(listofgradients,listofmodels,percent_vp,percent_vs,percent_rho,smoothfactor_x,smoothfactor_z,newfile,newfile_noheader):\n",
    "    model_update=assemble_lbfgs(listofgradients,listofmodels)\n",
    "    print('finish lbfgs')\n",
    "    x_new,z_new,vp,vs,rho=read_modelfile(listofmodels[-1])\n",
    "    smooth_format=smooth_compat(x_new,z_new,model_update[0],model_update[1],model_update[2],smoothfactor_x,smoothfactor_z)\n",
    "    print('finish smoothing')\n",
    "    vp_new=vp-percent_vp*smooth_format[0]\n",
    "    vs_new=vs-percent_vs*smooth_format[1]\n",
    "    rho_new=rho-percent_rho*smooth_format[2]\n",
    "    print(np.max(np.abs(percent_vp*smooth_format[0])),np.max(percent_vs*smooth_format[1]),np.max(percent_rho*smooth_format[2]))\n",
    "    generate_newmodel(vp_new,vs_new,rho_new,x_new,z_new,newfile,newfile_noheader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_source('EVENT1/DATA/SOURCE'))\n",
    "print(read_source('EVENT2/DATA/SOURCE'))\n",
    "print(read_source('EVENT3/DATA/SOURCE'))\n",
    "print(read_source('EVENT4/DATA/SOURCE'))\n",
    "print(read_source('EVENT5/DATA/SOURCE'))\n",
    "print(read_source('EVENT6/DATA/SOURCE'))\n",
    "print(read_source('EVENT7/DATA/SOURCE'))\n",
    "print(read_source('EVENT8/DATA/SOURCE'))\n",
    "print(read_source('EVENT9/DATA/SOURCE'))\n",
    "print(read_source('EVENT10/DATA/SOURCE'))\n",
    "print(read_source('EVENT11/DATA/SOURCE'))\n",
    "print(read_source('EVENT12/DATA/SOURCE'))\n",
    "print(read_source('EVENT13/DATA/SOURCE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "iteration='smooth'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 200)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels('smooth_summed_gradient.dat',show=False,EVENT='summed_smooth',total_max=0.5,figurename='summed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp smooth_summed_gradient.dat smooth_summed_gradient_tf.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "print('tf')\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_smooth/','ccc','DATA/STATIONS')\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_smooth/',modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "iteration='smooth'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 200)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp smooth_summed_gradient.dat smooth_summed_gradient_ccc.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels('smooth_summed_gradient_ccc.dat',show=False,EVENT='summed_smooth',total_max=1,figurename='summed_ccc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels('smooth_summed_gradient_tf.dat',show=False,EVENT='summed_smooth',total_max=0.5,figurename='summed_tf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "print('tf')\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_smooth/','waveform_misfit','DATA/STATIONS')\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_smooth/',modelfile)\n",
    "\n",
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "iteration='smooth'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 200)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp smooth_summed_gradient.dat smooth_summed_gradient_l2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "print('tf')\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_smooth/','cc_traveltime_misfit','DATA/STATIONS')\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_smooth/',modelfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "iteration='smooth'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 200)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp smooth_summed_gradient.dat smooth_summed_gradient_cc.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "list_events=['EVENT8']\n",
    "modelfile=PROJECT+'MODELS/profile_obs_smooth.xyz'\n",
    "print('tf')\n",
    "for _i, event in enumerate(list_events):\n",
    "    adj_calculate(PROJECT,event,'REF_SEM/','REF_SEM_smooth/','multitaper_misfit','DATA/STATIONS')\n",
    "    launch_adjoint(PROJECT,event,'REF_SEM_smooth/',modelfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events=['EVENT5','EVENT1','EVENT2','EVENT3','EVENT4','EVENT6','EVENT7','EVENT8','EVENT9','EVENT10','EVENT11','EVENT12','EVENT13']\n",
    "iteration='smooth'\n",
    "for _i,event in enumerate(list_events):\n",
    "    cut_source_region_from_gradient_gauss(event,iteration, 200)\n",
    "sum_gradient(list_events,iteration, smoothsrc='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp smooth_summed_gradient.dat smooth_summed_gradient_mt.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractkernelprofile(kernelfile):\n",
    "    data = np.loadtxt(kernelfile)\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    z2 = data[:,3] # e.g. alpha\n",
    "    X, Y, Z = grid(x,y,z2)\n",
    "    newX=X.swapaxes(0,1)\n",
    "    newZ=Z.swapaxes(0,1)\n",
    "    newY=Y.swapaxes(0,1)\n",
    "    return newX,newY,newZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX,newY,newZ=extractkernelprofile('smooth_summed_gradient_l2.dat')\n",
    "newX,newY,newZmt=extractkernelprofile('smooth_summed_gradient_mt.dat')\n",
    "newX,newY,newZcc=extractkernelprofile('smooth_summed_gradient_cc.dat')\n",
    "newX,newY,newZccc=extractkernelprofile('smooth_summed_gradient_ccc.dat')\n",
    "newX,newY,newZtf=extractkernelprofile('smooth_summed_gradient_tf.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('smooth_summed_gradient_mt.dat')\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "z2 = data[:,3] # e.g. alpha\n",
    "Xmt, Ymt, Zmt = grid(x,y,z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXmt=Xmt.swapaxes(0,1)\n",
    "newZmt=Zmt.swapaxes(0,1)\n",
    "newYmt=Ymt.swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newYmt.T[200-73]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newXmt.T[200-50],newZmt.T[200-50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile_smooth='MODELS/profile_obs_numpy_smooth.xyz'\n",
    "modelfile='MODELS/profile_obs_numpy.xyz'\n",
    "model_smooth=np.loadtxt(modelfile_smooth)\n",
    "model=np.loadtxt(modelfile)\n",
    "diff=model_smooth.T[2]-model.T[2]\n",
    "y_diff=model.T[1].reshape(200,720).swapaxes(0,1)[301]\n",
    "m_diff=diff.reshape(200,720).swapaxes(0,1)[301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.T[1].reshape(200,720).swapaxes(0,1).T[200-62]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 18])\n",
    "depth=np.linspace(900,0,len(y_diff))\n",
    "depth_kernel=np.linspace(800,0,len(newY[100]))\n",
    "modelfile_smooth='MODELS/profile_obs_numpy_smooth.xyz'\n",
    "modelfile='MODELS/profile_obs_numpy.xyz'\n",
    "model_smooth=np.loadtxt(modelfile_smooth)\n",
    "model=np.loadtxt(modelfile)\n",
    "diff=model_smooth.T[2]-model.T[2]\n",
    "y_diff=model.T[1].reshape(200,720).swapaxes(0,1)[301]\n",
    "m_diff=diff.reshape(200,720).swapaxes(0,1)[301]\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(depth,m_diff/m_diff.max(),'k',linewidth=10,alpha=0.5,label='model diff.')\n",
    "plt.plot(depth_kernel,newZ[100]/newZ[100].max(),'r',label='L2')\n",
    "plt.plot(depth_kernel,newZcc[100]/newZcc[100].max(),'b',label='CC time shift')\n",
    "plt.plot(depth_kernel,newZccc[100]/newZccc[100].max(),'g',label='CCC')\n",
    "plt.plot(depth_kernel,newZtf[100]/newZtf[100].max(),'m',label='TF phase shift')\n",
    "plt.plot(depth_kernel,newZmt[100]/newZmt[100].max(),'k',label='multitaper time shitf')\n",
    "plt.text(10,0.9,'(a)')\n",
    "plt.legend()\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Normalised Kernel (P wave)')\n",
    "plt.xlim(0,400)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(412)\n",
    "y_diff=model.T[1].reshape(200,720).swapaxes(0,1).T[174]\n",
    "m_diff=diff.reshape(200,720).swapaxes(0,1).T[174]\n",
    "#depth_kernel=np.linspace(0,30*111*1000,len(newX[100]))\n",
    "depth=np.linspace(0,111*30*1000,len(y_diff))\n",
    "plt.plot(depth/1000,m_diff/m_diff.max(),'k',linewidth=10,alpha=0.5)\n",
    "plt.plot(newX.T[200-29]/1000,newZmt.T[200-29]/newZmt.T[200-29].max(),'k')\n",
    "plt.plot(newX.T[200-29]/1000,newZcc.T[200-29]/newZcc.T[200-29].max(),'b')\n",
    "plt.plot(newX.T[200-29]/1000,newZccc.T[200-29]/newZccc.T[200-29].max(),'g')\n",
    "plt.plot(newX.T[200-29]/1000,newZ.T[200-29]/newZ.T[200-29].max(),'r')\n",
    "plt.plot(newX.T[200-29]/1000,newZtf.T[200-29]/newZtf.T[200-29].max(),'m')\n",
    "plt.text(100,0.9,'(b)')\n",
    "plt.xlabel('X (km)')\n",
    "plt.ylabel('Normalised Kernel (P wave)')\n",
    "plt.xlim(0,25*111)\n",
    "\n",
    "plt.subplot(413)\n",
    "y_diff=model.T[1].reshape(200,720).swapaxes(0,1).T[200-51]\n",
    "m_diff=diff.reshape(200,720).swapaxes(0,1).T[200-51]\n",
    "#depth_kernel=np.linspace(0,30*111*1000,len(newX[100]))\n",
    "depth=np.linspace(0,111*30*1000,len(y_diff))\n",
    "plt.plot(depth/1000,m_diff/m_diff.min(),'k',linewidth=10,alpha=0.5)\n",
    "plt.plot(newX.T[200-57]/1000,newZmt.T[200-57]/newZmt.T[200-57].min(),'k')\n",
    "plt.plot(newX.T[200-57]/1000,newZcc.T[200-57]/newZcc.T[200-57].min(),'b')\n",
    "plt.plot(newX.T[200-57]/1000,newZccc.T[200-57]/newZccc.T[200-57].min(),'g')\n",
    "plt.plot(newX.T[200-57]/1000,newZ.T[200-57]/newZ.T[200-57].min(),'r')\n",
    "plt.plot(newX.T[200-57]/1000,newZtf.T[200-57]/newZtf.T[200-57].min(),'m')\n",
    "plt.text(100,0.9,'(c)')\n",
    "plt.xlabel('X (km)')\n",
    "plt.ylabel('Normalised Kernel (P wave)')\n",
    "#plt.plot(depth,m_diff/m_diff.max(),'k',linewidth=10,alpha=0.5,label='model diff.')\n",
    "#plt.plot(depth_kernel,newZ[100]/newZ[100].max(),'r',label='L2')\n",
    "#plt.plot(depth_kernel,newZcc[100]/newZcc[100].max(),'b',label='CC time shift')\n",
    "#plt.plot(depth_kernel,newZccc[100]/newZccc[100].max(),'g',label='CCC')\n",
    "#plt.plot(depth_kernel,newZtf[100]/newZtf[100].max(),'m',label='TF phase shift')\n",
    "#plt.plot(depth_kernel,newZmt[100]/newZmt[100].max(),'k',label='multitaper time shitf')\n",
    "plt.legend()\n",
    "plt.xlim(0,25*111)\n",
    "\n",
    "plt.subplot(414)\n",
    "y_diff=model.T[1].reshape(200,720).swapaxes(0,1).T[200-62]\n",
    "m_diff=diff.reshape(200,720).swapaxes(0,1).T[200-62]\n",
    "#depth_kernel=np.linspace(0,30*111*1000,len(newX[100]))\n",
    "depth=np.linspace(0,111*30*1000,len(y_diff))\n",
    "plt.plot(depth/1000,m_diff/m_diff.max(),'k',linewidth=10,alpha=0.5)\n",
    "plt.plot(newX.T[200-69]/1000,newZmt.T[200-69]/newZmt.T[200-69].max(),'k')\n",
    "plt.plot(newX.T[200-69]/1000,newZcc.T[200-69]/newZcc.T[200-69].max(),'b')\n",
    "plt.plot(newX.T[200-69]/1000,newZccc.T[200-69]/newZccc.T[200-69].max(),'g')\n",
    "plt.plot(newX.T[200-69]/1000,newZ.T[200-69]/newZ.T[200-69].max(),'r')\n",
    "plt.plot(newX.T[200-69]/1000,newZtf.T[200-69]/newZtf.T[200-69].max(),'m')\n",
    "plt.xlabel('X (km)')\n",
    "plt.ylabel('Normalised Kernel (P wave)')\n",
    "plt.text(100,0.9,'(d)')\n",
    "plt.legend()\n",
    "plt.xlim(0,25*111)\n",
    "plt.savefig('Cross_kernel.pdf')\n",
    "\n",
    "\n",
    "\n",
    "#plt.xlim(0,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_kernel_different(modeldifffile,kernellist,misfitname,figname,amplitude):\n",
    "    fig, axes = plt.subplots(nrows=6, ncols=1,figsize=(15,18))\n",
    "    model=np.loadtxt(modeldifffile)\n",
    "    diff_3d=model.T[2].reshape(200,720)\n",
    "    axes[0].imshow((diff.T)*100,animated=True, cmap=cm.seismic_r, interpolation='nearest', vmin=-2, vmax=2,\\\n",
    "           extent=[0,30*111,0,900],aspect='auto')\n",
    "    \n",
    "    for _i,filename in enumerate(kernellist):\n",
    "        print(_i)\n",
    "        #ax.subplot()\n",
    "        data = np.loadtxt(filename)\n",
    "\n",
    "    # checks data\n",
    "        if data.ndim != 2:\n",
    "        #print \"Error: wrong data dimension for kernel file\",data.ndim\n",
    "            sys.tracebacklimit=0\n",
    "            raise Exception(\"Invalid data dimension\")\n",
    "\n",
    "    # checks array\n",
    "        if len(data[1,:]) != 5:\n",
    "        #print \"data shape  : \",data.shape\n",
    "        #print \"data lengths: \",len(data[:,1]),len(data[1,:])\n",
    "        #print \"Error: wrong data format for kernel file\",data.shape\n",
    "            sys.tracebacklimit=0\n",
    "            raise Exception(\"Invalid data format\")\n",
    "\n",
    "        x = data[:,0]\n",
    "        y = data[:,1]\n",
    "      #  z1 = data[:,2] # e.g. rho\n",
    "        z2 = data[:,3] # e.g. alpha\n",
    "       # z3 = data[:,4] # e.g. beta\n",
    "\n",
    "        X, Y, Z = grid(x,y,z2)\n",
    "        colormap = 'RdBu'\n",
    "        #print(np.max(np.abs(Z)))\n",
    "        #maxx=np.log10(np.max(np.abs(Z)))\n",
    "        \n",
    "\n",
    "        im = axes[_i+1].imshow(Z*amplitude[_i+1]/np.max(np.abs(Z)),vmax=1, vmin=-1, extent=[0,25*111,0,800],cmap=colormap,aspect='auto')\n",
    "               # im = ax.imshow(Z,vmax=total_max, vmin=total_min,extent=[0,45*111,0,1000],cmap=colormap.reversed())\n",
    "        axes[_i+1].plot([11*111,14*111,14*111,11*111,11*111],[140,140,230,230,140],'black')\n",
    "        axes[_i+1].plot(666,121,'o',color='black') #1\n",
    "        axes[_i+1].plot(666,221,'o',color='black') #2\n",
    "        axes[_i+1].plot(666,321,'o',color='black') #3\n",
    "        axes[_i+1].plot(1400,321,'o',color='black') #4\n",
    "        axes[_i+1].plot(2200,121,'o',color='black') #5 \n",
    "        axes[_i+1].plot(2200,221,'o',color='black') #6 \n",
    "        axes[_i+1].plot(2200,321,'o',color='black') #7\n",
    "        axes[_i+1].plot(1100,321,'o',color='black') #8\n",
    "        axes[_i+1].plot(1700,321,'o',color='black') #9\n",
    "        axes[_i+1].plot(1100,221,'o',color='black') #10\n",
    "        axes[_i+1].plot(1700,221,'o',color='black') #11\n",
    "        axes[_i+1].plot(1100,121,'o',color='black') #13\n",
    "        axes[_i+1].plot(1700,121,'o',color='black') #12\n",
    "        receiver_x=np.linspace(555,2220,60)\n",
    "        receiver_z=1e-3*np.zeros(60)\n",
    "        axes[_i+1].plot(receiver_x,receiver_z, 'vk',markersize=12)\n",
    "        #axes[_i].plot([0,45*111],[410,410],'black')\n",
    "       # axes[_i].plot([0,45*111],[660,660],'black')\n",
    "        #axes[_i].text(10,390,'410 km', fontsize=9)\n",
    "        #axes[_i].text(10,640,'660 km', fontsize=9)\n",
    "        axes[_i+1].set_ylim(0,400)\n",
    "        axes[_i+1].set_xlim(0,25*111)\n",
    "        axes[_i+1].invert_yaxis()\n",
    "        axes[_i+1].set_xlabel('X (km)',fontsize=14)\n",
    "        axes[_i+1].set_ylabel('Depth (km)',fontsize=14)\n",
    "        axes[_i+1].text(2000,365,misfitname[_i],fontsize=14)\n",
    "        axes[_i+1].text(100,300,'amplification factor:'+ str(amplitude[_i]), fontsize=14)\n",
    "        #axes[_i].set_xlim(1000,3500)\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist(),shrink=0.5,label='Normalised Summed Misfit Kernel (P wave)')\n",
    "    fig.savefig(figname)\n",
    "       #axes[_i].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernellist=['smooth_summed_gradient_tf.dat','smooth_summed_gradient_ccc.dat','smooth_summed_gradient_l2.dat','smooth_summed_gradient_cc.dat','smooth_summed_gradient_mt.dat']\n",
    "misfitname=['TF phase shift','CCC misfit','waveform L2','CC time shift','Multitaper time shift']\n",
    "amplitude=np.array([2,1,1.2,1,1.2])\n",
    "plot_kernel_different(kernellist,misfitname,'summed_kernel_compare_full.pdf',amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
